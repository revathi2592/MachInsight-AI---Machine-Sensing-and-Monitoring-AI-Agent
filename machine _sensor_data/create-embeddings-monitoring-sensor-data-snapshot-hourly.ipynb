{
  "cells": [
    {
      "cell_type": "code",
      "id": "SAiZdRSuY7SwmMCcDMwDEecT",
      "metadata": {
        "tags": [],
        "id": "SAiZdRSuY7SwmMCcDMwDEecT"
      },
      "source": [
        "from pymongo import MongoClient\n",
        "import pandas as pd\n",
        "from urllib.parse import quote_plus\n",
        "import vertexai\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "import os\n",
        "import tiktoken\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your Google Cloud project and location\n",
        "PROJECT_ID = \"apt-advantage-461615-m4\"\n",
        "PROJECT_LOCATION = \"us-central1\"\n",
        "vertexai.init(project=PROJECT_ID, location=PROJECT_LOCATION)"
      ],
      "metadata": {
        "id": "lSng-Cngjr7t"
      },
      "id": "lSng-Cngjr7t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MongoDB connection\n",
        "username = quote_plus(\"revathi2592\")\n",
        "password = quote_plus(\"Rev@thi2592\")\n",
        "mongo_uri = f\"mongodb+srv://{username}:{password}@cluster0.ihjjs0q.mongodb.net/?connectTimeoutMS=60000\"\n",
        "client = MongoClient(mongo_uri)\n",
        "db = client[\"Manufacturing_Sensor\"]\n",
        "collection = db[\"manufacturing-sensor-hourly-snapshot\"]"
      ],
      "metadata": {
        "id": "3mK9qQOtjuod"
      },
      "id": "3mK9qQOtjuod",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "docs = list(collection.find({}))\n",
        "df = pd.DataFrame(docs)\n",
        "df[\"date_hour\"] = df[\"hour\"].astype(\"int64\") // 10**9"
      ],
      "metadata": {
        "id": "jBg-VC7bjwPt"
      },
      "id": "jBg-VC7bjwPt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define columns for embedding\n",
        "features = [\n",
        "    \"avg_Error_Rate\",\n",
        "    \"avg_Power_Consumption_kW\",\n",
        "    \"avg_Production_Speed_units_per_hr\",\n",
        "    \"avg_temperature\",\n",
        "    \"avg_vibration\",\n",
        "    \"fault_probability\",\n",
        "    \"Faulty\",\n",
        "    \"date_hour\",\n",
        "    \"Machine_ID\",\n",
        "    \"record_count\"\n",
        "]"
      ],
      "metadata": {
        "id": "36VHo0exjyXt"
      },
      "id": "36VHo0exjyXt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values\n",
        "df = df.dropna(subset=features)"
      ],
      "metadata": {
        "id": "2oDoxg-sjzrV"
      },
      "id": "2oDoxg-sjzrV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create descriptive text for each row\n",
        "def row_to_text(row):\n",
        "    return (\n",
        "        f\"Machine {row['Machine_ID']} | \"\n",
        "        f\"Error Rate: {row['avg_Error_Rate']}, \"\n",
        "        f\"Power: {row['avg_Power_Consumption_kW']} kW, \"\n",
        "        f\"Speed: {row['avg_Production_Speed_units_per_hr']} units/hr, \"\n",
        "        f\"Temp: {row['avg_temperature']} C, \"\n",
        "        f\"Vibration: {row['avg_vibration']}, \"\n",
        "        f\"Fault Probability: {row['fault_probability']}, \"\n",
        "        f\"Faulty: {row['Faulty']}, \"\n",
        "        f\"DateHour: {row['date_hour']}, \"\n",
        "        f\"Record Count: {row['record_count']}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "QuCZSG_Zj1HN"
      },
      "id": "QuCZSG_Zj1HN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add embedding text and token count columns\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "MAX_TOKENS = 16000\n",
        "MAX_INSTANCES = 250"
      ],
      "metadata": {
        "id": "KOy7fFDLj2nO"
      },
      "id": "KOy7fFDLj2nO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"embedding_text\"] = df.apply(row_to_text, axis=1)\n",
        "df[\"token_count\"] = df[\"embedding_text\"].apply(lambda text: len(encoding.encode(text)))"
      ],
      "metadata": {
        "id": "e1ot65Z-j4kW"
      },
      "id": "e1ot65Z-j4kW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batching function with strict token limit\n",
        "def build_batches_strict(df, max_tokens=MAX_TOKENS, max_instances=MAX_INSTANCES):\n",
        "    batches = []\n",
        "    current_batch = []\n",
        "    current_token_count = 0\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        row_dict = row.to_dict()\n",
        "        text = row_dict[\"embedding_text\"]\n",
        "        token_count = row_dict[\"token_count\"]\n",
        "\n",
        "        # Truncate if a single row exceeds token limit\n",
        "        if token_count > max_tokens:\n",
        "            print(f\"Truncating row with {token_count} tokens to {max_tokens}\")\n",
        "            text = encoding.decode(encoding.encode(text)[:max_tokens])\n",
        "            token_count = len(encoding.encode(text))\n",
        "            row_dict[\"embedding_text\"] = text\n",
        "            row_dict[\"token_count\"] = token_count\n",
        "\n",
        "        # If adding this row would exceed limits, start a new batch\n",
        "        if (len(current_batch) >= max_instances) or (current_token_count + token_count > max_tokens):\n",
        "            if current_batch:\n",
        "                print(f\"Batch {len(batches)+1}: {len(current_batch)} records, {current_token_count} tokens\")\n",
        "                batches.append(current_batch)\n",
        "            current_batch = [row_dict]\n",
        "            current_token_count = token_count\n",
        "        else:\n",
        "            current_batch.append(row_dict)\n",
        "            current_token_count += token_count\n",
        "\n",
        "    if current_batch:\n",
        "        print(f\"Batch {len(batches)+1}: {len(current_batch)} records, {current_token_count} tokens\")\n",
        "        batches.append(current_batch)\n",
        "\n",
        "    return batches"
      ],
      "metadata": {
        "id": "N89tK6l7j6_2"
      },
      "id": "N89tK6l7j6_2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Further split any batch that still exceeds the token limit\n",
        "def split_batch_by_tokens(batch, max_tokens):\n",
        "    sub_batches = []\n",
        "    current_sub_batch = []\n",
        "    current_token_count = 0\n",
        "    for row in batch:\n",
        "        token_count = row[\"token_count\"]\n",
        "        if token_count > max_tokens:\n",
        "            # Truncate single row if needed\n",
        "            text = row[\"embedding_text\"]\n",
        "            print(f\"Truncating row with {token_count} tokens to {max_tokens}\")\n",
        "            text = encoding.decode(encoding.encode(text)[:max_tokens])\n",
        "            token_count = len(encoding.encode(text))\n",
        "            row[\"embedding_text\"] = text\n",
        "            row[\"token_count\"] = token_count\n",
        "        if current_token_count + token_count > max_tokens and current_sub_batch:\n",
        "            sub_batches.append(current_sub_batch)\n",
        "            current_sub_batch = [row]\n",
        "            current_token_count = token_count\n",
        "        else:\n",
        "            current_sub_batch.append(row)\n",
        "            current_token_count += token_count\n",
        "    if current_sub_batch:\n",
        "        sub_batches.append(current_sub_batch)\n",
        "    return sub_batches"
      ],
      "metadata": {
        "id": "GFW2xi0fj9qH"
      },
      "id": "GFW2xi0fj9qH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create initial batches\n",
        "batches = build_batches_strict(df)"
      ],
      "metadata": {
        "id": "q88NqunKj_QW"
      },
      "id": "q88NqunKj_QW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Further split batches if needed\n",
        "final_batches = []\n",
        "for batch in batches:\n",
        "    sub_batches = split_batch_by_tokens(batch, MAX_TOKENS)\n",
        "    final_batches.extend(sub_batches)"
      ],
      "metadata": {
        "id": "Z-ScnEHkkBBP"
      },
      "id": "Z-ScnEHkkBBP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")"
      ],
      "metadata": {
        "id": "Poac05dkkDX3"
      },
      "id": "Poac05dkkDX3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process batches and update MongoDB\n",
        "\n",
        "for batch_num, batch in enumerate(final_batches):\n",
        "    total_tokens = sum(row[\"token_count\"] for row in batch)\n",
        "    print(f\"Processing batch {batch_num + 1} with {len(batch)} records, {total_tokens} tokens\")\n",
        "    texts = [row[\"embedding_text\"] for row in batch]\n",
        "    try:\n",
        "        embeddings = model.get_embeddings(texts)\n",
        "        for i, row in enumerate(batch):\n",
        "            embedding = embeddings[i].values\n",
        "            collection.update_one({\"_id\": row[\"_id\"]}, {\"$set\": {\"embedding\": embedding}})\n",
        "    except Exception as e:\n",
        "        print(f\"Error in batch {batch_num + 1}: {e}\")\n",
        ""
      ],
      "metadata": {
        "id": "9mGowpRVkE73"
      },
      "id": "9mGowpRVkE73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vertex AI embeddings stored in MongoDB for each record.\")"
      ],
      "metadata": {
        "id": "FlpaJyyrkGSQ"
      },
      "id": "FlpaJyyrkGSQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "revathi2592 (Jun 15, 2025, 5:56:14â€¯PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
